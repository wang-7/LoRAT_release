### 添加新任务需要修改的地方

1. params.py中添加新任务的参数
2. train.py中choose_model函数中添加新任务的模型选择
3. dataset.py中from_path函数中添加新任务的数据集选择
4. diffusion.py中degrade_fn函数中添加新任务的降噪函数
5. federator.py中create_model函数中添加新任务的模型选择

10.16
1、**exp_10_16**该实验测试了用合成数据训练模型的可行性，用了较小数据集和较小模型，参数路径为/home/zuser/project/WQ/RF_Diffusion/RF-Diffusion/model/synthesis/exp_10_16/params.json，模型收敛良好，测试集上snr约为15
2、**exp_10_16_2**在上面实验基础上，用大数据集（10k）和大模型
收敛到1e-3数量级但波动较大，测试结果训练集snr约为10，测试集不足5，考虑到模型训练还是较慢，且所用训练集包含10个距离较远的位置，难以学习，下面考虑用1D_3311（set3）

10.17
1、改用1D_3311数据集，参数路径为/home/zuser/project/WQ/RF_Diffusion/RF-Diffusion/model/coordCSI/exp_10_17/params.json，结果未收敛

10.18
1、使用set5，与SANDiff对比实验，因服务器原因中断

2.13
1、**exp_fed_2_13**模型层数16，参数路径/media/zuser/Harddisk4TB/WQ/RF_Diffusion/RF-Diffusion/model/exp_fed_2_13/params.json，共训练200轮，收敛loss在0.001附近，用region0-6训练，在region0上测试snr为24.3，在region7上测试snr为12.9，region8上测试snr为17.5，region9上测试snr为24.1

2.14
1、**exp_fed_2_14**将模型层数改为32，其余参数与2.13相同，参数路径/media/zuser/Harddisk4TB/WQ/RF_Diffusion/RF-Diffusion/model/exp_fed_2_14/params.json，共训练150轮，收敛loss在0.001附近，用region0-6训练，在region0上测试snr为25.13，在region7上测试snr为11.9，region8上测试snr为16.4，在region9上测试snr为24.9。（注意这个测试结果是否说明区域对于信号特征有明显的影响）

2.16-2.17
1、**exp_fed_2_17**参数路径/media/zuser/Harddisk4TB/WQ/RF_Diffusion/RF-Diffusion/model/exp_FedVae_2_16/params.json和exp_FedVae_2_17/params.json，2.16实验用训练好的vae模型对训练数据进行压缩，然后用2.14实验的联邦架构训练，发现不收敛，后为验证是否为联邦架构的问题，进行2.17实验，该实验放弃联邦，用中心化训练，发现也不收敛，与2.16结果相似，loss在0.3附近震荡。考虑不是联邦的问题。

2.18
1、**exp_Vae_2_18**继续查找模型不收敛的原因，将数据集缩减为仅region0，模型层数减少为16，其余与2.17相同，结果仍不收敛。注意到去年10月所做实验中，对于用坐标预测CSI的任务，是能够收敛的，而argos中用上行预测下行的实验是不能收敛的，考虑到两者区别在于输入的condition是否是经过vae随机采样的，因此考虑不收敛的原因是不能将vae的随机采样作为condition输入模型。

2、**exp_Vae_2_18_2**

2.26
新idea：用联邦加LoRA，先全参数训练，然后对每个用户用LoRA微调，再将LoRA微调的参数拿来迁移。

2.27
希望先做一个验证性实验，跑通lora，并测试lora的效果。先用2.13实验的方法得到联邦聚合后的模型，拿到新区域测试，然后用lora在新区域微调，比较两种方法的结果。
1、**exp_LoRA_2_27**该实验采用非联邦，简单用region0\~6的数据训练模型，最后loss收敛到1e-5数量级，在region0上的SNR为39.20，region7上SNR为36.80，region8上SNR为38.59，region9上SNR为37.46。接下来希望将模型放在region7\~9上微调，观察是否有提升。

2、**exp_LoRA_2_27_2**将region7~9按7：3分为训练集和测试集，测试exp_LoRA_2_27模型在测试集上的结果：region7_test 37.04；region8_test 38.67；region9_test 37.26。将exp_LoRA_2_27的模型在region7-9_train上分别微调后，在各自测试集上测试结果：
在region7_train上微调 region7_test:42.75 region8_test:38.20 region9_test:36.35
在region8_train上微调 region7_test:37.60 region8_test:44.48 region9_test:37.99
在region9_train上微调 region7_test:36.79 region8_test:38.81 region9_test：40.80

3.3
1、实验计划：先不考虑联邦场景，用实验exp_LoRA_2_27的模型，分别拿到各client上微调，然后将微调后的lora矩阵聚合拿到新client上测试，与未微调过的模型比较
**exp_LoRA_3_3_0-6**将模型在region0-6上分别微调，得到6个lora矩阵。根据uamp（文件路径/media/zuser/Harddisk4TB/WQ/bedroom/exp_2_12_FedDiff/umap.ipynb）降维结果计算出在region7-9上的聚合权重，将训练好的6个lora矩阵分别聚合，得到三个新lora矩阵，用这些矩阵分别在region7-9上测试结果
region7 43.14
region8 44.98
region9 43.86

3.11
**exp_Argos_3_7**
该实验在argos数据集上训练，将数据集按用户划分，在用户1~7上训练，探究迁移到用户0上的效果。训练时间4.853h，loss降到0.0023，测试结果平均SNR为
**FT_1**全局模型：23.3
**FT_2**将上述训练好的模型放在用户2数据集上微调，训练时间3.4h，loss降到0.0022，测试结果平均SNR为27.44，对比全局模型测试结果为25.84。

3.12
继续测试合成数据集上的泛化性，首先测试各个性化模型在本地和新地点的SNR以及全局模型在新地点的SNR。使用的模型是exp_LoRA_3_4，unseen[3, 4, 6]
region0: 48.92
region1: 接近50
region2：接近50
region3: 全局模型下10.94, 直接平均lora矩阵：10.95，用legos方案聚合：11.4
region4:全局模型下38.41，注意因为region4和region5数据相似，因此用全局模型效果较好
region6：全局模型下10.97，直接平均lora矩阵：11.1，legos方案：11.15

3.14

**exp_Argos_3_14**

测试只用单天线的argos数据的效果，先在用户1~7上训练，用时5h，收敛到0.004.

4.4
**exp_4_4_multiband**
用合成数据进行的多频带实验，一个频段预测另外五个频段，第一次实验因为cond和data顺序没有对应未收敛，第二次实验成功。数据路径：/media/zuser/Harddisk4TB/WQ/bedroom/multi_band

4.7
**exp_4_7_multibandZHR**
用实测数据进行的实验，进行了多次实验，最终使用的是第二次实验。第二次实验是先将数据平滑后，划分为长度为20的样本，用前20预测后20。参见/media/zuser/Harddisk4TB/WQ/bedroom/multi_band_ZHR/smooth.ipynb。训练完成后使用fast_sampling对所有样本有序采样，保存在/media/zuser/Harddisk4TB/WQ/RF_Diffusion/RF-Diffusion/dataset/exp_4_7_2_multibandZHR/output。之后将样本拼接在一起，下载到本地绘图。参见check.ipynb。本地绘图地址：D:\BaiduSyncdisk\project\plot_animation

4.19
通过观察argos数据集的umap低维映射，发现数据集ArgosCSI-96x8-2016-11-04-05-37-37_2.4GHz_track_left_to_right_NLOS.npy的ant_idx=5的client0和6、7之间有相似性。考虑可以用6和7的模型迁移到0上去提高性能。

**exp_Argos_4_19**
用上述数据集ant5、client1-7进行训练全局模型，训练8小时以上，四小时左右收敛，在1-7训练集上测试结果snr普遍接近30（未划分测试集），在client0上测试结果平均snr为4.77 

**exp_Argos_4_20**
用上述数据集ant5、client6-7单独训练，在6-7训练集上测试snr普遍接近30，在client0上测试结果平均snr为4.34

4.27
使用来自Argos中不同场景下运动客户端的数据构建一个迁移环境，数据集分析参见/media/zuser/Harddisk4TB/WQ/Raw-CSI-Data/check.ipynb。用该数据集进行了两组实验，仅参与客户端不同，其余不变。
**exp_Argos_4_27**
参与客户端为[0, 1, 3, 4, 5]，训练4h，loss收敛到0.01左右，在训练集上测试的平均SNR为21.02，在未参与训练的客户端2上平均SNR为11.53
**exp_Argos_4_27_2**
参与客户端为[0, 1, 2, 3, 5]，训练4h，loss收敛到0.01左右，在训练集上测试的平均SNR为21.09，在客户端4（未参与训练）上测试平均SNR为10.29

4.28
后续设计实验需要改进：划分测试集，注意时间长度是14，应当先按时间切片再划分。采用联邦框架？
**exp_Argos_4_29**
重新划分了测试集重复实验exp_Argos_4_27，数据集为/media/zuser/Harddisk4TB/WQ/Raw-CSI-Data/set1_4_27，seen_idx = [0, 1, 3, 4, 5]，注意为了提高效率，将嵌入维度减小为128，层数减小为12/12。训练时间5h，loss收敛到0.01左右。

**exp_Argos_4_29_2**
实验条件不变，seen_idx=[0, 1, 2, 3, 5]，训练时间5h，loss收敛到0.01左右。继续进行fine-tune实验，在五个客户端上分别进行微调，训练时间8.4h。loss结果如下
Client0：0.0042
Client1：0.0133
Client2：0.0054
Client3：0.0135
Client5：0.0009
可见，模型在0，2，5上微调有更明显的效果，而在1，3上loss与全局模型相比没有明显降低。在测试集上的测试结果（平均SNR），第一列为个性模型，第二列为全局模型
Client0：16.12  15.11
Client1：6.27  6.13
Client2：14.47  13.66
Client3：5.16  5.10
Client5：31.79  28.33
据观察，不同client上效果的巨大差异源自数据在时间上变化的快慢（训练集和测试集的差异）。接着还需要测试全局模型和各个个性化模型在client4（未参与训练）上的测试结果
全局：9.95
FT0：10.24
FT1：10.12
FT2：10.41
FT3：10.10
FT5：9.56
FT0和FT2聚合：10.14
虽然各FT模型提升有限，但相对大小符合预期。需要改进：首先client1和3上测试效果太差，要么换变化慢的数据集要么尝试减小时间长度。其次LoRA相对全局模型的提升效果不够明显，考虑增大lora_r，

4.30
**exp_Argos_4_30**
替换掉上面实验中表现不好的两个数据集，新数据集为/media/zuser/Harddisk4TB/WQ/Raw-CSI-Data/set2_4_29。全局模型训练10.65h，收敛到0.0016。在五个客户端上微调1.6h的loss如下：
Client0:0.0019
Client1:0.0002
Client2:0.0028
Client3:0.0013
Client5:0.0003
(ant_idx=2)在测试集上的测试结果SNR，第一二列个性化，第三四列全局
Client0：19.62(Ave.) 19.83(Med.) 19.03(Ave.) 19.51(Med.) 
Client1：17.59(Ave.) 14.39(Med.) 15.73(Ave.) 1.87(Med.)
Client2：16.44(Ave.) 17.00(Med.) 15.85(Ave.) 17.16(Med.)
Client3：24.60(Ave.) 25.02(Med.) 23.37(Ave.) 23.80(Med.)
Client5：16.96(Ave.) 17.54(Med.)(微调矩阵为weights-1080.pt) 15.33(Ave.) 12.21(Med.)
下面是全局模型和各个性化模型在client4上的结果
全局：9.73(Ave.) 11.71(Med.)
FT0：9.58(Ave.) 11.60(Med.)
FT1：9.69(Ave.) 11.71(Med.)
FT2：9.67(Ave.) 11.65(Med.)
FT3：9.73(Ave.) 11.83(Med.)
FT5：9.71(Ave.) 11.73(Med.)

W0: avg 16.25 med 17.20
2source: avg 16.58 med 17.55
3source: avg 16.43 med 17.44
4source: avg 16.25 med 17.20

Client3 other antennas
ant snr
3 10.34
7 12.54

**exp_Argos_5_1**
验证lora rank的影响。选取Client1, 上面已经有了r=8的数据，还需要做r=1，2，4，16和全参数微调五组实验。
下面是各组在Client1测试集上的结果
r1: 17.45(Ave.) 14.39(Med.)
r2: 17.71(Ave.) 14.09(Med.)
r4: 17.83(Ave.) 16.21(Med.)
r16: 18.05(Ave.) 24.27(Med.)
full: 17.56(Ave.) 17.43(Med.)

**2025_8_8**
补充实验，考虑添加合成数据集，用不同的多普勒频移区分不同的区域。能否更进一步，直接采用一个信道模型而改变其某参数来实现不同区域。

**exp_Argos_8_21**
使用数据集WQ/Raw-CSI-Data/set2_4_29，与4_30实验相同，只是改为全天线参与训练，参与训练的基站为0，1，2，4，5。收敛良好，训练5.4小时loss降至约0.007。
W0在参与训练的基站上测试结果：avg 21.0，med 21.3
W0在基站0上测试结果：avg 19.5 med 20.5
W0在基站1上测试结果：avg 24.9 med 25.1
W0在基站2上测试结果：avg 18.2 med 18.9
W0在基站3上测试结果：avg -1.4 med -1.3
W0在基站4上测试结果：avg 16.7 med 17.2
W0在基站5上测试结果：avg 25.7 med 26.0

**exp_Argos_8_22**
将上述W0在基站3上微调，目的是探究微调能多大程度上使模型适应新分布。lora rank=16
10h训练，loss只能降到0.9，说明在新的分布上微调效果有限。

**exp_Argos_8_23**
将九个Argos文件的第一个client（动态）和第二个client（静态）提取出来，组成共18个用户。将模型在[0, 1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14]上训练。目的是希望通过加大训练数据量（包含动态和静态的数据），能够提升w0的泛化性，另外保留了3个动态和3个静态场景的数据作为后续微调和迁移的目标。
训练7h，loss降到0.05，注意到比8_21实验高很多。

W0在参与训练的基站上测试结果：avg 16.3，med 18.2
W0在基站0上测试结果：avg 14.4 med 14.2
W0在基站1上测试结果：avg 4.0 med 3.9
W0在基站2上测试结果：avg 14.0 med 14.4
W0在基站3上测试结果：avg 3.3 med 2.8
W0在基站4上测试结果：avg 12.4 med 12.4
W0在基站5上测试结果：avg 20.0 med 20.0

W0在基站9上测试结果：avg 20.9 med 22.5
W0在基站10上测试结果：avg 20.0 med 21.1
W0在基站11上测试结果：avg 21.9 med 22.3
W0在基站12上测试结果：avg 21.5 med 22.0
W0在基站13上测试结果：avg 19.1 med 19.6
W0在基站14上测试结果：avg 23.7 med 24.4

W0在基站6上测试结果：avg 6.9 med 1.2 （注意到样本两极化严重，要么15左右，要么-1~-0）
W0在基站7上测试结果：avg -1.2 med -1.2 （全是负数）
W0在基站8上测试结果：avg -0.9 med -0.9 （全是负数）

W0在基站15上测试结果：avg 18.7 med 18.9 （表现非常好，甚至好于在参与训练的基站上的总体结果）
W0在基站16上测试结果：avg -0.7 med -0.7 （全是负数）
W0在基站17上测试结果：avg -0.8 med -0.9 （全是负数）

后在[6, 7, 8, 15, 16, 17]上微调后再测试，
FT6在基站6上测试结果：avg 7.2 med 4.1 （没有负数，部分2~3，部分10以上）
FT7在基站7上测试结果：avg 0.0 med 0.0 （非常接近0）
FT8在基站8上测试结果：avg 0.2 med 0.2

FT15在基站15上测试结果：avg 11.9 med 12.0
FT16在基站16上测试结果：avg 0.4 med 0.5
FT17在基站17上测试结果：avg 0.3 med 0.3

后在[0, 1, 3, 5, 13, 14]上微调后再测试
FT0在基站0上测试结果：avg 14.6 med 14.3
FT1在基站1上测试结果：avg 4.0 med 3.9
FT3在基站3上测试结果：avg 3.4 med 2.9
FT5在基站5上测试结果：avg 20 med 20
FT13在基站13上测试结果：avg 19.3 med 20.0
FT14在基站14上测试结果：avg 23.6 med 24.4

**exp_vivo_9_15**
将模型在vivo数据集上训练，由于代码疏漏，只选取了每个扇区下的一个用户，模型收敛良好，4h降至9e-4。

**exp_vivo_9_16**
更正代码，用全部400个用户训练。
注意到如果将400个用户全部放在第一个维度，每轮需要的训练时间11min，且收敛过慢，4h降至0.6且波动较大。考虑其他方案。

**exp_vivo_9_16_2**
将用户维度放在子载波维，重复上面实验。1.5h降至0.78，基本收敛，曲线较为平滑。
以上两组实验表明由于用户数过多，导致分布差异太大，超出模型能力。

**exp_vivo_9_17**
改为仅用5用户，并加大参数规模，模型可以收敛，最终loss在0.1附近
训练集上snr在个位数，测试集上大部分是负数。考虑为模型无法泛化，可能是因为时间切片是不重叠切片

**exp_vivo_9_17_2**
将切片改为滑动窗口切片。收敛到0.03，测试集多数个位数，少部分负数，训练集类似。
由于改为滑动窗口，数据量大大增加，因此考虑只用一个用户。

**exp_vivo_9_18**
改为只用一个用户（数据集不变，修改dataset文件），7h收敛至7e-3。在测试集上snr在20左右。说明之前用户数（数据量）超出模型容量。
本实验使用了sector 1-15作为不同的源域，下面测试模型在未知域上的表现。
sector 15: avg -4.22 med -4.68
sector 16: avg -0.99 med -1.01
sector 17: avg -0.68 med -0.71
sector 18: avg -2.80 med -3.49
sector 19: avg -2.04 med -2.11
sector 20: avg -2.31 med -2.26

**exp_vivo_10_5**
